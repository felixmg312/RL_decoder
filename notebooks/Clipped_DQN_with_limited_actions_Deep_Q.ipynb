{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "010abdbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/felixmeng/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pyemd import emd\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download('stopwords')\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import torch \n",
    "from collections import namedtuple, deque\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from transformers import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9834c5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-base\"\n",
    "embedding_model=gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "pretrained_model=AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "pretrained_tokenizer=AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deffa5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "    def __init__(self,input_sentence,model_name,sentence,reward):\n",
    "        \"\"\"\n",
    "        input_sentence: the input sentence x \n",
    "        model_name: the transformer model that we are using\n",
    "        sentence: the sentence class that contains helper function for the currently decoded word\n",
    "        reward: the reward class that will return the reward of a new action\n",
    "        \"\"\"\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        input_ids = self.tokenizer(input_sentence,return_token_type_ids=False,return_tensors='pt').input_ids\n",
    "        self.input_ids = input_ids\n",
    "        self.decoder_input_ids = torch.tensor([[self.model.config.decoder_start_token_id]])\n",
    "        outputs = self.model(input_ids, decoder_input_ids= self.decoder_input_ids, return_dict=True)\n",
    "        self.encoded_sequence = (outputs.encoder_last_hidden_state,)\n",
    "        self.lm_logits = outputs.logits\n",
    "        self.word_embed = self.model.state_dict()['model.encoder.embed_tokens.weight']\n",
    "        self.input_sentence=input_sentence\n",
    "        ## additional stuff to call from \n",
    "        self.reward=reward()\n",
    "        self.sentence_class=sentence()\n",
    "\n",
    "    def action(self,action_word_id):\n",
    "        \"\"\"\n",
    "        Returns next state given an action word\n",
    "        \"\"\"\n",
    "        next_decoder_input_ids = torch.tensor([[action_word_id]])\n",
    "        self.decoder_input_ids = torch.cat([self.decoder_input_ids, next_decoder_input_ids], axis=-1)\n",
    "        self.lm_logits  = self.model(None, encoder_outputs= self.encoded_sequence, decoder_input_ids= self.decoder_input_ids, return_dict=True).logits\n",
    "        return {\n",
    "            'word_embeddings': [self.word_embed[i,:] for i in self.decoder_input_ids[0]],\n",
    "            'logits': self.lm_logits\n",
    "          }\n",
    "    \n",
    "    def generated_sentence_so_far(self):\n",
    "        return self.tokenizer.decode(self.decoder_input_ids[0], skip_special_tokens = True)\n",
    "    \n",
    "    def encoded_input(self):\n",
    "        return self.encoded_sequence[0]\n",
    "    \n",
    "    def reference_decode(self):\n",
    "        #generate reference target from greedy,beam, topk.\n",
    "        #maybe add more \n",
    "        greedy_output = self.model.generate(self.input_ids)\n",
    "\n",
    "        beam_output = self.model.generate(\n",
    "              self.input_ids,\n",
    "              num_beams=1, \n",
    "              no_repeat_ngram_size=2, \n",
    "              early_stopping=True\n",
    "      )\n",
    "        topk_output = self.model.generate(\n",
    "              self.input_ids, \n",
    "              do_sample=True, \n",
    "              top_k=50,\n",
    "              temperature=0.7\n",
    "      )\n",
    "        return [\n",
    "            self.tokenizer.decode(greedy_output[0], skip_special_tokens=True),\n",
    "            self.tokenizer.decode(beam_output[0], skip_special_tokens=True),\n",
    "            self.tokenizer.decode(topk_output[0], skip_special_tokens=True)\n",
    "        ]\n",
    "    ## a bunch of classifiers for a given sentence\n",
    "    def fluency_score(self,action):\n",
    "        pass\n",
    "    def sentiment_class(self,action):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def step(self,action):\n",
    "        \"\"\"\n",
    "        returns next_state, reward, termination\n",
    "        \"\"\"\n",
    "        next_state= self.action(action)\n",
    "        termination= self.sentence_class.is_termination()\n",
    "        reward= self.reward.evaluate_reward()\n",
    "        return next_state,reward,termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d460ba2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaf00c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78453c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62f1d04c882844d1aa3da1f997a0c60c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ce1321d34314064a8992fa5883a755c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/687 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aec2fab00274359b10e8579763b42e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ca9ee55e9e4cb3ba9e3f6a0f63beb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b23122c6f1247f88299199c69747c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067b4fcb1e514b0abf1fe468a77e226f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5689d6055d64a55b01f11af1e5373b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.99888676404953}]\n"
     ]
    }
   ],
   "source": [
    "def sentiment_pred(list_of_text):\n",
    "    logging.set_verbosity_error()\n",
    "    pipe = pipeline(model=\"siebert/sentiment-roberta-large-english\")\n",
    "    # pipe = pipeline(model=\"roberta-large-mnli\")\n",
    "    return pipe(list_of_text)\n",
    "\n",
    "print(sentiment_pred([\"I really like this one\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0927c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44105269",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reward():\n",
    "    def __init__(self,embedding_model,action,target_sentence,end_token,stopwords,discount=1,done=False):\n",
    "        self.embedding_model=embedding_model\n",
    "        self.action=action\n",
    "        self.target_sentence=target_sentence\n",
    "        self.end_token=end_token\n",
    "        self.stopwords=stopwords\n",
    "        self.discount=discount\n",
    "        self.done=False\n",
    "        self.simulated_sentence=None\n",
    "    def simulate_sentence(self,sentence,action,idx,prob,max_iteration=10):\n",
    "        Sentence=sentence()\n",
    "        sentence.update_sentence(action,idx,prob)\n",
    "        for _ in range(k):\n",
    "            ## todo: env gives idx,prob\n",
    "            self.simulated_sentence=sentence.simulate_sentence(idx,prob)\n",
    "    def evaluate_reward(self):\n",
    "        ## todo: waiting for the classifiers\n",
    "        pass\n",
    "    def sentence_mover_distance(self,generated_sentence,base_line_sentence,target_sentence):\n",
    "        target_sentence = target_sentence.lower().split()\n",
    "        base_line_sentence= base_line_sentence.lower().split()\n",
    "        print(generated_sentence,base_line_sentence, target_sentence)\n",
    "\n",
    "        generated_sentence = [w for w in generated_sentence if w not in self.stopwords]\n",
    "        target_sentence = [w for w in target_sentence if w not in self.stopwords]\n",
    "        base_line_sentence=[w for w in base_line_sentence if w not in self.stopwords]\n",
    "        generated_distance= self.model.wmdistance(generated_sentence, target_sentence)\n",
    "        baseline_distance= self.model.wmdistance(base_line_sentence, target_sentence)\n",
    "        print(\"baseline_distance\",baseline_distance,\"generated distance\",generated_distance)\n",
    "        return baseline_distance,generated_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce6c53c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e35d677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Sentence():\n",
    "    def __init__(self,start_token=\"<s>\",end_token=\"<\\s>\"):\n",
    "        \"\"\"\n",
    "        Initialize the sentence with a start token\n",
    "        \"\"\"\n",
    "        self.sentence=[start_token]\n",
    "        self.possible_actions={\"add_word\":0,\"replace_word\":1,\"remove_word\":2}\n",
    "        self.end_token=end_token\n",
    "        self.start_token=start_token\n",
    "    def get_sentence(self,sentence):\n",
    "        \"\"\"sentence getter\"\"\"\n",
    "        return self.sentence\n",
    "    def sample_string(self,idx,value):\n",
    "        \"\"\"sampling the next possible word given the likelihood\"\"\"\n",
    "        assert(len(idx)==len(value))\n",
    "        self.next_word=np.random.choice(idx, p=prob)\n",
    "        return self.next_word\n",
    "    def update_sentence(self,action,idx,prob):\n",
    "        \"\"\"\n",
    "        update the sentence according to the given list of possible actions {add_word,replace_word,remove_word}\n",
    "        \"\"\"\n",
    "        if action==\"add_word\":\n",
    "            word=self.sample_string(idx,prob)\n",
    "            self.sentence.append(word)\n",
    "        if action== \"replace_word\":\n",
    "            self.sentence[-1]=self.next_word\n",
    "        if action== \"delete_word\":\n",
    "            self.sentence.pop()\n",
    "    def isTermination():\n",
    "        \"\"\"\n",
    "        if is termination end\n",
    "        \"\"\"\n",
    "        return self.sentence[-1]==self.end_token\n",
    "    def simulate(self,idx,prob):\n",
    "        \"\"\"\n",
    "        idx= words with different probabilities\n",
    "        prob= probablity of those different words\n",
    "        \"\"\"\n",
    "        action=random.choice(list(self.possible_actions.keys()))\n",
    "        idx=[\"i\",\"like\",\"to\",\"eat\"]\n",
    "        prob=[0.1,0.2,0.3,0.4]\n",
    "        self.update_sentence(action,idx,prob)\n",
    "        return self.sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff9b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c7953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec702d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "144a047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition= namedtuple('Transition',('state', 'action', 'next_state', 'reward','termination'))\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Used deque so that if our counter is greater than maxlen, the previous will be removed\n",
    "        \"\"\"\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition with each corresponding state\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        sample a random sample from the batch\n",
    "        return 'state', 'action', 'next_state', 'reward','termination'\n",
    "        \"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return current length of deque\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f612a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN():\n",
    "    \"\"\"\n",
    "    Initialized a basic neural network dimension\n",
    "    \"\"\"\n",
    "    def __init__(self,n_actions,input_dim,fc1_dims=3000,fc2_dims=4000,lr=5e-5):\n",
    "        super(DQN, self).__init__()\n",
    "        self.n_actions=n_actions\n",
    "        self.input_dim=input_dim\n",
    "        self.lr=lr\n",
    "        self.fc1=nn.Linear(input_dim,fc1_dims)\n",
    "        self.fc2=nn.Linear(fc1_dims,fc2_dims)\n",
    "        self.fc3=nn.Linear(fc2_dims,n_actions)\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.optimizer= optim.Adam(self.parameters(),lr=lr)\n",
    "        self.loss= nn.HuberLoss()\n",
    "        self.device= T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "    def forward(self,state):\n",
    "        x=self.flatten(state)\n",
    "        x= F.relu(self.fc1(state))\n",
    "        x= F.relu(self.fc2(x))\n",
    "        actions = self.fc3(x)\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3a3ea090",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Using Clipped double Q learning, so we have two deep Q networks and we are always updating with the min of the two Q values\n",
    "    \"\"\"\n",
    "    def __init__(self,env,DQN,replay_memory,learning_rate=3e-4,gamma=0.99, buffer_size=1000):\n",
    "        self.env=env()\n",
    "        self.learning_rate= learning_rate\n",
    "        self.gamma= gamma\n",
    "        self.buffer_size=buffer_size\n",
    "        self.replay_buffer= ReplayMemory(buffer_size)\n",
    "        self.device=torch.device(\"cuda\" if torch.cuda.is_availabe() else \"cpu\")\n",
    "        ## \n",
    "        self.model1=DQN(n_actions=3,input_dim=10).to(self.device) \n",
    "        self.model2=DQN(n_actions=3,input_dim=10).to(self.device) \n",
    "  \n",
    "        self.optimizer1 = torch.optim.Adam(self.model1.parameters())\n",
    "        self.optimizer2 = torch.optim.Adam(self.model2.parameters())\n",
    "    def get_action(self,state,eps=0.2):\n",
    "        qvals=self.model1.forward(state)\n",
    "        action= np.argmax(qvals.cpu().detach().numpy())\n",
    "        \n",
    "        if(np.random.randn()<eps):\n",
    "            return self.env.action_space.sample()\n",
    "    def compute_loss(self,batch_size):\n",
    "        ## reading from batch and reinitializing the state\n",
    "        states, actions, rewards, next_states, terminations= self.replay_buffer.sample(batch_size)[0]\n",
    "        states=torch.FloatTensor(states).to(self.device)\n",
    "        actions=torch.LongTensor(actions).to(self.device)\n",
    "        rewards=torch.FloatTensor(rewards).to(self.device)\n",
    "        next_states= torch.FloatTensor(next_states).to(self.device)\n",
    "        terminations=torch.FloatTensor(terminations)\n",
    "        \n",
    "        ## possible resizing necessary\n",
    "        \n",
    "        \"\"\"\n",
    "        compute the current-state-Q-values and the next-state-Q-values of both models,\n",
    "        but use the minimum of the next-state-Q-values to compute the expected Q value\n",
    "        \"\"\"\n",
    "        curr_Q1= self.model1.forward(states)\n",
    "        curr_Q2= self.model2.forward(states)\n",
    "        next_Q1= self.model1.forward(next_states)\n",
    "        next_Q2= self.model2.forward(next_states)\n",
    "        \n",
    "        next_Q= torch.min(\n",
    "            torch.max(next_Q1,1)[0],\n",
    "            torch.max(next_Q2,1)[0]\n",
    "        )\n",
    "        \n",
    "        next_Q= next_Q.view(next_Q.size(0),1)\n",
    "        expected_Q= rewards+ (1-dones)*self.gamam*next_Q\n",
    "        \n",
    "        \n",
    "        loss1= F.huber_loss(curr_Q1, expected_Q.detach())\n",
    "        loss2= F.huber_loss(curr_Q2, expected_Q.detach())\n",
    "        return loss1,loss2\n",
    "    def update(self,batch_size):\n",
    "        loss1,loss2=self.compute_loss(batch_size)\n",
    "        self.optimizer1.zero_grad()\n",
    "        loss1.backward()\n",
    "        self.optimizer1.step()\n",
    "        self.optimizer2.zero_grad()\n",
    "        loss2.backward()\n",
    "        self.optimizer2.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "db020812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env,agent,max_epochs,max_steps,batch_size):\n",
    "    \"\"\"\n",
    "    env: need to implement step\n",
    "    agent:\n",
    "    max_epochs: max training length\n",
    "    max_steps: max sequence length of our sentence\n",
    "    batch_size: the batch that we are taking for each training epoch\n",
    "    \"\"\"\n",
    "    epoch_rewards=[]\n",
    "    for epoch in range(max_epochs):\n",
    "        #state= env.reset()\n",
    "        epoch_reward=0\n",
    "        for step in range(max_steps):\n",
    "            action= agent.get_action(state)\n",
    "            next_state,reward,done=env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            epoch_reward+=reward\n",
    "            \n",
    "            if len(agent.replay_buffer) > batch_size:\n",
    "                agent.update(batch_size)\n",
    "            if done or step == max_steps-1:\n",
    "                epoch_rewards.append(epoch_reward)\n",
    "                print(\"epoch\" + str(epoch) + \": \" + str(epoch_reward))\n",
    "                break\n",
    "            state= next_state\n",
    "    return epoch_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "744e9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Reader:\n",
    "    def __init__(self, data_name=\"gigaword\", test_size=0.1, data_set_size=20000,mode=\"training\"):\n",
    "        \"\"\"\n",
    "        data set reader\n",
    "        \"\"\"\n",
    "        if mode==\"training\":\n",
    "            dataset=load_dataset(data_name,split = 'train')\n",
    "            train_data, test_data= dataset.train_test_split(test_size=test_size).values()\n",
    "            small_dataset= datasets.DatasetDict({'train':train_data,'test':test_data})\n",
    "            self.input_sentences=small_dataset['train']['document'][:data_set_size]\n",
    "            self.output_sentences=small_dataset['train']['summary'][:data_set_size]\n",
    "        if mode== \"testing\":\n",
    "            train_ds, test_ds = datasets.load_dataset(data_name, split=['train', 'test'])\n",
    "            ## todo:: filling this up\n",
    "    def get_input(self):\n",
    "        return self.input_sentences\n",
    "    def get_output(self):\n",
    "        return self.output_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3658c4a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
